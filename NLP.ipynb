{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "06f5bcea-dcab-41eb-851a-b72d0e7c4a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLP Study Plan\n",
    "# Author: Oscar Tibaduiza\n",
    "# Date: 23/05/2024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "44c53d42-ad0d-46d6-a2d8-6a16adbe7283",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Introduction\n",
    "# This notebook presents a study plan to learn Natural Language Processing (NLP). \n",
    "# The goal is to provide a structured guide covering basic concepts to more advanced topics.\n",
    "\n",
    "# ------------------------------\n",
    "# 1. Fundamentals of NLP\n",
    "# ------------------------------\n",
    "\n",
    "# 1.1. Introduction to NLP\n",
    "# - What is NLP?\n",
    "# - Applications of NLP\n",
    "# - Popular tools and libraries (NLTK, spaCy, Hugging Face Transformers)\n",
    "\n",
    "# 1.2. Text Preprocessing\n",
    "# - Tokenization\n",
    "# - Normalization (lowercasing, lemmatization, stemming)\n",
    "# - Stop words removal\n",
    "# - Regular expressions for text cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4eb27dc9-ce59-45d0-b218-12efa4ac1dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Practical exercise: Tokenization and basic preprocessing\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5e047a97-8583-45de-b978-95f0e5dd1d0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\oscar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\oscar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\oscar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download necessary resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "85ae4f0b-dbee-4dcf-80c5-44d58cd4ea00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example text\n",
    "text = \"Hello! How are you? This is a sample text for basic preprocessing.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dcb122cd-2fe5-415a-8c6c-3057da8d9362",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: ['hello', '!', 'how', 'are', 'you', '?', 'this', 'is', 'a', 'sample', 'text', 'for', 'basic', 'preprocessing', '.']\n"
     ]
    }
   ],
   "source": [
    "# Tokenization\n",
    "tokens = word_tokenize(text.lower())\n",
    "print(\"Tokens:\", tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a3de4705-52d5-46d3-bacd-1fefd05d6254",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens without stopwords: ['hello', '!', '?', 'sample', 'text', 'basic', 'preprocessing', '.']\n"
     ]
    }
   ],
   "source": [
    "# Stop words removal\n",
    "stop_words = set(stopwords.words('english'))\n",
    "tokens_without_stopwords = [word for word in tokens if word not in stop_words]\n",
    "print(\"Tokens without stopwords:\", tokens_without_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eafadfa0-a05f-46f4-9332-c8a1acda8fc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatized tokens: ['hello', '!', '?', 'sample', 'text', 'basic', 'preprocessing', '.']\n"
     ]
    }
   ],
   "source": [
    "# Lemmatization\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatized_tokens = [lemmatizer.lemmatize(word) for word in tokens_without_stopwords]\n",
    "print(\"Lemmatized tokens:\", lemmatized_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d9887898-0bef-4d67-b8a7-4b1183d0f90e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------\n",
    "# 2. Text Representation\n",
    "# ------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "af2730f1-da1f-43ba-8200-28732d02b5db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.1. Bag of Words (BoW)\n",
    "# - Concept of bag of words\n",
    "# - Building a bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bc6a9222-ec73-41a3-be93-7ed33f4e98e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bag of Words:\n",
      " [[0 1 1 1 0 0 1 0 1]\n",
      " [0 2 0 1 0 1 1 0 1]\n",
      " [1 0 0 1 1 0 1 1 1]\n",
      " [0 1 1 1 0 0 1 0 1]]\n",
      "Features:\n",
      " ['and' 'document' 'first' 'is' 'one' 'second' 'the' 'third' 'this']\n"
     ]
    }
   ],
   "source": [
    "# Practical exercise: Create a bag of words\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "corpus = [\n",
    "    \"This is the first document.\",\n",
    "    \"This document is the second document.\",\n",
    "    \"And this is the third one.\",\n",
    "    \"Is this the first document?\"\n",
    "]\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "print(\"Bag of Words:\\n\", X.toarray())\n",
    "print(\"Features:\\n\", vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fb1cf876-ebdb-4acf-8d3a-33885bd42a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.2. TF-IDF\n",
    "# - Concept of TF-IDF\n",
    "# - Implementation of TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8c9d0fc5-6dcd-4c18-bfca-0a7a8d607d8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF Matrix:\n",
      " [[0.         0.46979139 0.58028582 0.38408524 0.         0.\n",
      "  0.38408524 0.         0.38408524]\n",
      " [0.         0.6876236  0.         0.28108867 0.         0.53864762\n",
      "  0.28108867 0.         0.28108867]\n",
      " [0.51184851 0.         0.         0.26710379 0.51184851 0.\n",
      "  0.26710379 0.51184851 0.26710379]\n",
      " [0.         0.46979139 0.58028582 0.38408524 0.         0.\n",
      "  0.38408524 0.         0.38408524]]\n",
      "TF-IDF Features:\n",
      " ['and' 'document' 'first' 'is' 'one' 'second' 'the' 'third' 'this']\n"
     ]
    }
   ],
   "source": [
    "# Practical exercise: Create a TF-IDF matrix\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "X_tfidf = tfidf_vectorizer.fit_transform(corpus)\n",
    "print(\"TF-IDF Matrix:\\n\", X_tfidf.toarray())\n",
    "print(\"TF-IDF Features:\\n\", tfidf_vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "171762f9-bbad-44f6-a061-d8953f33cc8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------\n",
    "# 3. Language Models\n",
    "# ------------------------------\n",
    "\n",
    "# 3.1. N-gram Models\n",
    "# - Introduction to n-grams\n",
    "# - Implementation of n-gram models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "87281d50-8052-4d28-bf93-00a43e9820b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bigrams: [('hello', '!'), ('!', 'how'), ('how', 'are'), ('are', 'you'), ('you', '?'), ('?', 'this'), ('this', 'is'), ('is', 'a'), ('a', 'sample'), ('sample', 'text'), ('text', 'for'), ('for', 'basic'), ('basic', 'preprocessing'), ('preprocessing', '.')]\n"
     ]
    }
   ],
   "source": [
    "# Practical exercise: Generate n-grams\n",
    "from nltk import ngrams\n",
    "\n",
    "bigrams = list(ngrams(tokens, 2))\n",
    "print(\"Bigrams:\", bigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b3dc8d80-0a94-41f1-a0c4-f2c583b9c86e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.2. Word Embeddings\n",
    "# - Concept of word embeddings\n",
    "# - Introduction to Word2Vec and GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9769fe1d-4a4c-4d2f-98c3-5e9dda23e21f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.10.0\n"
     ]
    }
   ],
   "source": [
    "import scipy\n",
    "print(scipy.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d637f410-7181-4493-9737-930d5a945c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.linalg import triu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "342191ed-225d-4772-99af-0e8a7c21f459",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector for 'document': [-1.07023621e-03  4.75227891e-04  1.02121495e-02  1.80161148e-02\n",
      " -1.86066683e-02 -1.42376255e-02  1.29124559e-02  1.79410204e-02\n",
      " -1.00314002e-02 -7.53031159e-03  1.47636728e-02 -3.07031744e-03\n",
      " -9.07182693e-03  1.31084993e-02 -9.72215272e-03 -3.63255502e-03\n",
      "  5.75860823e-03  1.98871316e-03 -1.65728815e-02 -1.88926701e-02\n",
      "  1.46204550e-02  1.01437848e-02  1.35136731e-02  1.52757065e-03\n",
      "  1.27034336e-02 -6.80697383e-03 -1.89409894e-03  1.15419105e-02\n",
      " -1.50391106e-02 -7.87741225e-03 -1.50275519e-02 -1.86425447e-03\n",
      "  1.90778263e-02 -1.46398256e-02 -4.67134174e-03 -3.87989124e-03\n",
      "  1.61594115e-02 -1.18607190e-02  8.52437806e-05 -9.51008499e-03\n",
      " -1.92053970e-02  1.00120399e-02 -1.75162610e-02 -8.78598820e-03\n",
      " -6.87807769e-05 -5.96724800e-04 -1.53191071e-02  1.92321129e-02\n",
      "  9.96452942e-03  1.84679516e-02]\n"
     ]
    }
   ],
   "source": [
    "# Practical exercise: Train a Word2Vec model\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "sentences = [word_tokenize(doc.lower()) for doc in corpus]\n",
    "word2vec_model = Word2Vec(sentences, vector_size=50, window=5, min_count=1, workers=4)\n",
    "print(\"Vector for 'document':\", word2vec_model.wv['document'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e1a4ab96-532a-4853-ab58-69f56c4a4a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------\n",
    "# 4. Advanced Models and Applications\n",
    "# ------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "622d60b2-5a14-4848-bf91-b7b928f1cf94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.1. Transformers and BERT\n",
    "# - Introduction to transformers\n",
    "# - Basic implementation of BERT for text classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0e07ceec-430b-4e38-9e61-97d0bd9a081c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.16.1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f1d1c0a0-237d-44cb-b423-84a3564704c5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.0+cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "548a7ce2-42b5-4d55-beda-cb19aeac4058",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\oscar\\Anaconda\\envs\\NLP\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\oscar\\Anaconda\\envs\\NLP\\Lib\\site-packages\\huggingface_hub\\file_download.py:157: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\oscar\\.cache\\huggingface\\hub\\models--distilbert--distilbert-base-uncased-finetuned-sst-2-english. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification result: [{'label': 'POSITIVE', 'score': 0.9998804330825806}]\n"
     ]
    }
   ],
   "source": [
    "# Practical exercise: Text classification with BERT\n",
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline('sentiment-analysis')\n",
    "result = classifier(\"This NLP course is amazing!\")\n",
    "print(\"Classification result:\", result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6376670f-66cc-46bc-97a0-f41670b228b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.2. Applications of NLP\n",
    "# - Sentiment analysis\n",
    "# - Text summarization\n",
    "# - Machine translation\n",
    "# - Chatbots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "dd70df59-0a43-4315-a695-9e4a09cc8dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------\n",
    "# 5. Final Projects\n",
    "# ------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bf06e477-75ac-4bb8-ae6b-76f893726315",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.1. Project 1: Sentiment Analysis\n",
    "# - Collect social media data\n",
    "# - Preprocess data\n",
    "# - Train a sentiment analysis model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3257a92c-48f1-4ff1-94be-111d67f7b735",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.2. Project 2: Text Generation\n",
    "# - Implement a text generation model\n",
    "# - Train the model with a large corpus\n",
    "# - Evaluate the quality of the generated text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "43750a0f-88ce-49a5-a709-8c321a2bf689",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------\n",
    "# Additional Resources\n",
    "# ------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a1826c0c-a4d8-413c-aa93-59786eb196d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# - Recommended books and articles\n",
    "# - Online courses and tutorials\n",
    "# - Code repositories and datasets\n",
    "\n",
    "# End of the study plan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "088e0330-1be5-4233-ad84-2548dc1722a5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
